{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de modelo Watson Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumário\n",
    "\n",
    "* [1. Bibliotecas para importação](#biblio)\n",
    "* [2. Preparação do data frame](#prep)\n",
    "  * [Limpeza dos logs](#limp)\n",
    "* [3. Extração de amostra](#amos)\n",
    "  * [Arquivo para curadoria](#arq_cura)\n",
    "* [4. Data frame de curadoria](#df_cura)\n",
    "* [5. Acurácia](#acur)\n",
    "* [6. Matriz de confusão](#matrix)\n",
    "* [7. Relatório de classificação (precisão, recall, F1-score)](#relat)\n",
    "  * [Tabela de F1-score](#f1_score)\n",
    "  * [Precisão X recall](#prec_recal)\n",
    "* [8. Análise de confiança](#conf)\n",
    "* [9. Cálculo do limite de confiança (τ)](#conf_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"biblio\"></a>1. Bibliotecas para importação\n",
    "Instala, importa e atualiza bibliotecas e funções necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from amostragem import get_sample_size, sample_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"prep\"></a>2. Preparação do data frame\n",
    "Abre arquivo de logs extraídos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch1 = \"EXTRACAO.CSV\" #indicar arquivo .csv com a extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.read_csv(arch1)\n",
    "logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id=\"limp\"></a>Limpeza do data frame\n",
    "Seleciona logs de primeiro turno, irrelevantes, logs com inputs vazios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turn1_logs = logs[logs.turn_counter == 1] #logs de primeiro turno\n",
    "irrelevant_logs = logs[logs.intent == \"Irrelevant\"] #logs classificados como Irrelevantes\n",
    "null_input_logs = logs[logs['input'].isnull()] #logs com inputs vazios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = set(list(irrelevant_logs.index) + list(null_input_logs.index))\n",
    "clean_logs = logs.drop(to_remove).reset_index(drop=True)\n",
    "#clean_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"amos\"></a>3. Extração de amostra\n",
    "Usa função programada para extração de amostra dos logs para curadoria humana.\n",
    "Amostra baseada em 95% de índice de confiança e 5% de margem de erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = len(clean_logs)\n",
    "sample_size = get_sample_size(pop)\n",
    "sample_table = sample_extractor(clean_logs, sample_size)\n",
    "sample_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"arq_cura\"></a>Arquivo para curadoria \n",
    "Salva arquivo .csv local para curadoria e classificação manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table.to_csv(str(datetime.now()) + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  <a id=\"df_cura\"></a>Data frame de curadoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch2 = \"cura1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_logs = pd.read_csv(arch2, dtype={'check': str})\n",
    "curated_logs.check.astype(str)\n",
    "for index, row in curated_logs.iterrows():\n",
    "    if row['intent_CM'] == row['intent_watson']:\n",
    "        curated_logs.at[index, 'score'] = 1\n",
    "        curated_logs.at[index, 'check'] = \"S\"\n",
    "    else:\n",
    "        curated_logs.at[index, 'score'] = 0\n",
    "        curated_logs.at[index, 'check'] = \"N\"\n",
    "\n",
    "curated_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id=\"acur\"></a>Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy_data = {\"S\":[curated_logs[curated_logs.check == \"S\"][\"check\"].count()],\n",
    "                  \"N\":[curated_logs[curated_logs.check == \"N\"][\"check\"].count()]}\n",
    "accuracy_df = pd.DataFrame(data=accuracy_data)\n",
    "accuracy_df['S'].plot.bar(figsize=(3, 5), bottom=accuracy_df['N'])\n",
    "accuracy_df['N'].plot.bar(color='red').set_yticks([accuracy_data['N'][0], accuracy_data['N'][0]+accuracy_data['S'][0]])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(\"Acuracy:\", round(accuracy_df[\"S\"][0]/(accuracy_df[\"S\"][0]+accuracy_df[\"N\"][0]), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a id=\"matrix\"></a>Matriz de confusão "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_intents = list(curated_logs['intent_CM'])\n",
    "wt_intents = list(curated_logs['intent_watson'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = curated_logs['intent_CM'].drop_duplicates().sort_values() #cria labels com as intenções testadas\n",
    "output_matrix = confusion_matrix(cm_intents, wt_intents, labels=labels) #cria matriz de confusão\n",
    "index_labels  = ['CM:{:}'.format(x) for x in labels] #label das intenções classificadas manualmente\n",
    "column_labels = ['Watson:{:}'.format(x) for x in labels]#lavel das intenções classificadas pelo Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "df_cm = pd.DataFrame(output_matrix, index=index_labels, columns=column_labels) #cria DF da matriz de confusão\n",
    "hm = sns.heatmap(df_cm, cmap=\"binary\", annot=True, cbar=False, linewidths=.01, linecolor='white', square=True)\n",
    "hm.set_yticklabels(hm.get_yticklabels(), rotation=0)\n",
    "hm.set_xticklabels(hm.get_xticklabels(), rotation=90)\n",
    "hm.set_frame_on(True)\n",
    "plt.title(\"Matriz de confusão\")\n",
    "plt.tight_layout()\n",
    "plt.autoscale()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"relat\"></a>7. Relatório de classificação (precisão, recall, F1-score)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(cm_intents, wt_intents, output_dict=True) #cria relatório de indicadores de performance de modelo\n",
    "cr_df = pd.DataFrame(cr).T\n",
    "cr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"f1_score\"></a>Tabela de F1-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cr).T[\"f1-score\"].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"prec_recal\"></a>Precisão x recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scatter = cr_df.drop(index=[\"accuracy\", \"macro avg\", \"weighted avg\"]).sort_values(\"support\", ascending=False)\n",
    "#scatter.plot.scatter(\"precision\", \"recall\", figsize=(7,7))\n",
    "sizer = {key:value*30 for key, value in scatter['support'].items()}\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "    \n",
    "plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "\n",
    "for index, row in scatter.iterrows():\n",
    "    if row[\"f1-score\"] == 0:\n",
    "        continue #remove intenções com suporte = 0\n",
    "    plt.scatter(row['precision'], row['recall'], label=index, s=sizer[index])\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.title(\"Classification Report\")\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), title=\"INTENÇÕES\", labelspacing=4, ncol= 2, handlelength=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"conf\"></a>8. Análise de confiança\n",
    "Distribuição dos acertos e erros do modelo por índice de confiança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_conf_list = curated_logs[curated_logs['check']=='N']['confidence']\n",
    "yes_conf_list = curated_logs[curated_logs['check']=='S']['confidence']\n",
    "bins = np.linspace(0, 1, 6)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.hist([no_conf_list, yes_conf_list], bins=bins, label=['incorreto', 'correto'], color=['red', 'royalblue'])\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Distribuição de logs por confiança')\n",
    "plt.xlabel('Confiança')\n",
    "plt.ylabel('Qts de logs')\n",
    "plt.show()\n",
    "\n",
    "curated_logs.boxplot(column=\"confidence\", by=\"check\", figsize=(3, 5), showfliers=False, grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixa assertividade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curated_logs[curated_logs['check'] == 'N'][curated_logs[\"confidence\"] <= 0.85]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"conf_calc\"></a>9. Cálculo do limite de confiança (τ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_results = []\n",
    "\n",
    "correct_weight = 1\n",
    "incorrect_weight = -1.5\n",
    "ignored_weight = 0\n",
    "\n",
    "print(\"Total:\", len(curated_logs), \"logs\")\n",
    "\n",
    "for i in range(0, 100):\n",
    "    test_tau = i/100\n",
    "    \n",
    "    correct_count = len(curated_logs[(curated_logs['check'] == 'S') & (curated_logs['confidence'] >= test_tau)])\n",
    "    incorrect_count = len(curated_logs[(curated_logs['check'] == 'N') & (curated_logs['confidence'] >= test_tau)])\n",
    "    ignored_count = len(curated_logs[(curated_logs['confidence'] < test_tau)])\n",
    "    \n",
    "    score = ((correct_count*correct_weight)+(incorrect_count*incorrect_weight)+(ignored_count*ignored_weight))\n",
    "    \n",
    "    threshold_result = (test_tau,score)\n",
    "    threshold_results.append(threshold_result)\n",
    "    #print('test_tau:', threshold_result[0], \"score:\", threshold_result[1])\n",
    "\n",
    "optimum_condfidence_threshold, max_score = max(threshold_results, key=itemgetter(1))\n",
    "print('optimum_condfidence_threshold:', optimum_condfidence_threshold)\n",
    "print('max_score:', max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intenções conflitantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conflict_intents = {\"Intents\":[],\n",
    "                   \"Qtd\":[]}\n",
    "agrupado = curated_logs[(curated_logs['check'] == 'N')].sort_values(['intent_CM', 'intent_watson'])[[\"intent_CM\", \"intent_watson\", \"input\"]]\n",
    "for index, row in agrupado.iterrows():\n",
    "    key = row['intent_CM'] + \" / \" + row['intent_watson']\n",
    "    if key in conflict_intents[\"Intents\"]:\n",
    "        conflict_intents[\"Qtd\"][conflict_intents[\"Intents\"].index(key)] += 1\n",
    "    if key not in conflict_intents[\"Intents\"]:\n",
    "        conflict_intents[\"Intents\"].append(key)\n",
    "        conflict_intents[\"Qtd\"].append(1)\n",
    "#conflict_intents\n",
    "conflicted_intents = pd.DataFrame(data=conflict_intents).sort_values(\"Qtd\", ascending=False) #Tabela com pares de intenções conflitantes\n",
    "conflicted_intents[conflicted_intents[\"Qtd\"] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_train = curated_logs[(curated_logs[\"intent_CM\"] != \"Irrelevant\") & (curated_logs[\"intent_watson\"] != \"Irrelevant\") & (curated_logs['check'] == 'N')][[\"input\", \"intent_CM\"]]\n",
    "to_train.to_csv(\"treinamento.csv\", index=False) #salva local planilha com exemplos para treinamento de intenções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "curated_logs[curated_logs.intent_CM == \"Irrelevant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
